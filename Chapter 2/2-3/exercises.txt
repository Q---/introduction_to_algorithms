2.3-1
Using Figure 2.4 as a model, illustrate the operation of merge sort on the array A = {3, 41, 52, 26, 38, 57, 9, 49}.

3
    3 41
41
            3 26 41 52
52
    26 52
26
                        3 9 26 38 41 49 52 57
38
    38 57
57
            9 38 49 57
9
    9 49
49

2.3-2
Rewrite the MERGE procedure so it does not use sentinels, instead stopping once either L or R has had all its elements copied back to A, and then copying the remainder of the other array back into A.

MERGE(A, p, q, r)
    n_1 = q - p + 1
    n_2 = r - q
    let L[1..n_1 + 1] and R[1..n_2 + 1] be new arrays
    for i = 1 to n_1
        L[i] = A[p + i - 1]
    for j = 1 to n_2
        R[j] = A[q + j]
    i = 1
    j = 1
    for k = p to r and i < n_1 and j < n_2
        if L[i] <= R[j]
            A[k] = L[i]
            i = i + 1
        else A[k] = R[j]
            j = j + 1
    if i != n_1
        for k to r
            A[k] = L[i]
            i = i + 1
    elseif j != n_2
	for k to r
            A[k] = R[j]
            j = j + 1

2.3-3
Use mathematical induction to show that when n is an exact power of 2, the solution of the recurrence:
T(n) = 2 if n=2; 2T(n/2)+n if n=2^k, for k > 1
is T(n) = nlg(n)

For the proof, see the corresponding image file

2.3-4
We can express insertion sort as a recursive procedure as follows.  In order to sort A[1 .. n], we recursively sort A[1 .. n-1] and then insert A[n] into the sorted array A[1 .. n-1].  Write a recurrence for the running time of this recursive version of insertion sort.

The recurrence is

       { 1           if n = 1
T(n) = {
       { T(n-1) + n  if n > 1

The solution to this recurrence is n(n+1)/2, leading to O(n^2) time complexity.

For the proof, see the corresponding image file.

Pseudocode for recursive insertion sort (modeled as two functions):

// Recursively sorts subarray A[1 .. n], O(n^2)
insertSort(A, n)
1    if n > 1
2        insertSort(A, n-1)
3        insert(A, n)

// Inserts A[n] into place on sorted subarray A[1 .. n-1], O(n)
insert(A, n)
1    A_n = A[n]
2    i = n - 1
3    while i > 0 and A[i] > A_n
4        A[i + 1] = A[i]
5        i = i - 1
6    A[i + 1] = A_n

2.3-5
Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence A is sorted, we can check the midpoint of the sequence against v and eliminate half of the sequence from further consideration.  The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time.  Write pseudocode, either iterative or recursive, for binary search.  Argue that the worst-case running time of binary search is O(lg n).

binarySearch(A, key, p, r)
1    if p < r
2        q = floor((p+r)/2)
3        if A[q] < key
4            return binarySearch(A, key, p, q-1)
5        else if A[q] > key
6            return binarySearch(A, key, q, r)
7        else return q
8    else return NIL

Divide: Dividing (in this case, spawn one subproblem) the problem consists of a constant number of comparisons, so this contributes O(1)

Conquer: The problem becomes half the size of the original, contributing T(n/2) time cost on each recurse.

Combine: Determining if the current index has the key is constant time, so O(1)

Therefore,

       { 1           if n = 2
T(n) = { 
       { T(n/2) + 1  if n = {2^k | k is in Z+}

Which is equivalent to ln n

For the proof, see the corresponding image file.

2.3-6













